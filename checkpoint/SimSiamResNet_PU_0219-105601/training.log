SGD optimizer for the classifier 

02-19 10:56:01 model_name: SimSiamResNet
02-19 10:56:01 data_name: PU
02-19 10:56:01 aug_1: normal
02-19 10:56:01 aug_2: randomcrop
02-19 10:56:01 max_epoch: 20
02-19 10:56:01 data_view: TwoViewDataset
02-19 10:56:01 cuda_device: 0
02-19 10:56:01 checkpoint_dir: ./checkpoint
02-19 10:56:01 batch_size: 32
02-19 10:56:01 data_dir: raw_data/PU
02-19 10:56:01 out_channel: 4
02-19 10:56:01 normlizetype: minus_one_one
02-19 10:56:01 processing_type: RA
02-19 10:56:01 opt: sgd
02-19 10:56:01 lr: 0.01
02-19 10:56:01 momentum: 0.9
02-19 10:56:01 weight_decay: 1e-05
02-19 10:56:01 lr_scheduler: cos
02-19 10:56:01 gamma: 0.1
02-19 10:56:01 eta_min: 1e-05
02-19 10:56:01 task: self_supervised
02-19 10:56:01 critetion: <class 'utils.loss_SSL.SimSiamLoss'>
02-19 10:56:01 using 1 gpus
02-19 10:56:01 Dataset class: <class 'data_utils.datasets.PU.PU'>
02-19 10:56:02 Split sizes: train=22502 val=4822 test=4823
02-19 10:56:03 Label counts train: Counter({1: 8481, 3: 7718, 0: 4201, 2: 2102})
02-19 10:56:04 Label counts val:   Counter({1: 1817, 3: 1654, 0: 900, 2: 451})
02-19 10:56:04 Label counts test:  Counter({1: 1818, 3: 1654, 0: 901, 2: 450})
02-19 10:56:08 -----Epoch 0/19-----
02-19 10:56:36 current lr: 0.01
02-19 10:56:36 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9461)
02-19 10:56:36 Epoch 000 Train loss -0.9214 | Val loss -0.9461
02-19 10:56:36 -----Epoch 1/19-----
02-19 10:57:01 current lr: 0.0099385
02-19 10:57:01 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9916)
02-19 10:57:01 Epoch 001 Train loss -0.9778 | Val loss -0.9916
02-19 10:57:01 -----Epoch 2/19-----
02-19 10:57:28 current lr: 0.00975553
02-19 10:57:28 Epoch 002 Train loss -0.9900 | Val loss -0.9894
02-19 10:57:28 -----Epoch 3/19-----
02-19 10:57:59 current lr: 0.00945558
02-19 10:57:59 Epoch 003 Train loss -0.9910 | Val loss -0.9855
02-19 10:57:59 -----Epoch 4/19-----
02-19 10:58:32 current lr: 0.00904604
02-19 10:58:32 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9917)
02-19 10:58:32 Epoch 004 Train loss -0.9936 | Val loss -0.9917
02-19 10:58:32 -----Epoch 5/19-----
02-19 10:59:00 current lr: 0.008537
02-19 10:59:00 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9941)
02-19 10:59:00 Epoch 005 Train loss -0.9949 | Val loss -0.9941
02-19 10:59:00 -----Epoch 6/19-----
02-19 10:59:33 current lr: 0.00794099
02-19 10:59:33 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9957)
02-19 10:59:33 Epoch 006 Train loss -0.9959 | Val loss -0.9957
02-19 10:59:33 -----Epoch 7/19-----
02-19 11:00:02 current lr: 0.00727268
02-19 11:00:02 Epoch 007 Train loss -0.9964 | Val loss -0.9943
02-19 11:00:02 -----Epoch 8/19-----
02-19 11:00:31 current lr: 0.00654854
02-19 11:00:31 Epoch 008 Train loss -0.9952 | Val loss -0.9943
02-19 11:00:31 -----Epoch 9/19-----
02-19 11:01:01 current lr: 0.00578639
02-19 11:01:01 Epoch 009 Train loss -0.9948 | Val loss -0.9938
02-19 11:01:01 -----Epoch 10/19-----
02-19 11:01:34 current lr: 0.005005
02-19 11:01:34 Epoch 010 Train loss -0.9957 | Val loss -0.9951
02-19 11:01:34 -----Epoch 11/19-----
02-19 11:02:06 current lr: 0.00422361
02-19 11:02:06 Epoch 011 Train loss -0.9965 | Val loss -0.9938
02-19 11:02:06 -----Epoch 12/19-----
02-19 11:02:42 current lr: 0.00346146
02-19 11:02:42 Epoch 012 Train loss -0.9961 | Val loss -0.9939
02-19 11:02:42 -----Epoch 13/19-----
02-19 11:03:18 current lr: 0.00273732
02-19 11:03:18 Epoch 013 Train loss -0.9961 | Val loss -0.9932
02-19 11:03:18 -----Epoch 14/19-----
02-19 11:03:54 current lr: 0.00206901
02-19 11:03:54 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9963)
02-19 11:03:54 Epoch 014 Train loss -0.9966 | Val loss -0.9963
02-19 11:03:54 -----Epoch 15/19-----
02-19 11:04:30 current lr: 0.001473
02-19 11:04:30 Epoch 015 Train loss -0.9968 | Val loss -0.9956
02-19 11:04:30 -----Epoch 16/19-----
02-19 11:05:05 current lr: 0.00096396
02-19 11:05:05 Epoch 016 Train loss -0.9968 | Val loss -0.9933
02-19 11:05:05 -----Epoch 17/19-----
02-19 11:05:41 current lr: 0.000554422
02-19 11:05:41 Epoch 017 Train loss -0.9969 | Val loss -0.9950
02-19 11:05:41 -----Epoch 18/19-----
02-19 11:06:16 current lr: 0.000254473
02-19 11:06:16 Epoch 018 Train loss -0.9969 | Val loss -0.9953
02-19 11:06:16 -----Epoch 19/19-----
02-19 11:06:53 current lr: 7.14967e-05
02-19 11:06:53 Epoch 019 Train loss -0.9971 | Val loss -0.9954
02-19 11:06:55 TEST (last): loss -0.9953
02-19 11:07:13 [LP epoch 00] train_loss=1.2727 train_acc=0.3778 | val_loss=6124.0597 val_acc=0.3770 | 
02-19 11:07:27 [LP epoch 01] train_loss=1.2623 train_acc=0.3833 | val_loss=6131.7748 val_acc=0.3731 | 
02-19 11:07:41 [LP epoch 02] train_loss=1.2589 train_acc=0.3849 | val_loss=6123.2834 val_acc=0.3772 | 
02-19 11:07:55 [LP epoch 03] train_loss=1.2607 train_acc=0.3833 | val_loss=6122.9175 val_acc=0.3781 | 
02-19 11:08:09 [LP epoch 04] train_loss=1.2600 train_acc=0.3846 | val_loss=6119.3968 val_acc=0.3787 | 
02-19 11:08:23 [LP epoch 05] train_loss=1.2594 train_acc=0.3853 | val_loss=6114.7976 val_acc=0.3795 | 
02-19 11:08:36 [LP epoch 06] train_loss=1.2609 train_acc=0.3846 | val_loss=6121.9430 val_acc=0.3787 | 
02-19 11:08:51 [LP epoch 07] train_loss=1.2590 train_acc=0.3864 | val_loss=6127.6989 val_acc=0.3756 | 
02-19 11:09:05 [LP epoch 08] train_loss=1.2592 train_acc=0.3865 | val_loss=6130.2587 val_acc=0.3770 | 
02-19 11:09:19 [LP epoch 09] train_loss=1.2591 train_acc=0.3863 | val_loss=6115.6901 val_acc=0.3785 | 
02-19 11:09:34 [LP epoch 10] train_loss=1.2597 train_acc=0.3851 | val_loss=6137.5928 val_acc=0.3768 | 
02-19 11:09:48 [LP epoch 11] train_loss=1.2591 train_acc=0.3851 | val_loss=6119.0828 val_acc=0.3785 | 
02-19 11:10:03 [LP epoch 12] train_loss=1.2589 train_acc=0.3839 | val_loss=6114.8199 val_acc=0.3776 | 
02-19 11:10:17 [LP epoch 13] train_loss=1.2581 train_acc=0.3841 | val_loss=6113.5002 val_acc=0.3776 | 
02-19 11:10:31 [LP epoch 14] train_loss=1.2574 train_acc=0.3856 | val_loss=6121.7184 val_acc=0.3756 | 
02-19 11:10:45 [LP epoch 15] train_loss=1.2555 train_acc=0.3852 | val_loss=6098.0455 val_acc=0.3776 | 
02-19 11:10:59 [LP epoch 16] train_loss=1.2571 train_acc=0.3860 | val_loss=6118.3373 val_acc=0.3764 | 
02-19 11:11:14 [LP epoch 17] train_loss=1.2561 train_acc=0.3873 | val_loss=6117.0191 val_acc=0.3770 | 
02-19 11:11:28 [LP epoch 18] train_loss=1.2567 train_acc=0.3874 | val_loss=6122.5405 val_acc=0.3774 | 
02-19 11:11:43 [LP epoch 19] train_loss=1.2571 train_acc=0.3863 | val_loss=6209.9216 val_acc=0.3743 | 
02-19 11:11:57 [LP epoch 20] train_loss=1.2560 train_acc=0.3859 | val_loss=6112.2464 val_acc=0.3787 | 
02-19 11:12:11 [LP epoch 21] train_loss=1.2557 train_acc=0.3885 | val_loss=6101.3148 val_acc=0.3791 | 
02-19 11:12:25 [LP epoch 22] train_loss=1.2550 train_acc=0.3909 | val_loss=6096.2498 val_acc=0.4027 | 
02-19 11:12:39 [LP epoch 23] train_loss=1.2534 train_acc=0.3915 | val_loss=6094.3336 val_acc=0.3787 | 
02-19 11:12:54 [LP epoch 24] train_loss=1.2582 train_acc=0.3905 | val_loss=6124.2413 val_acc=0.3764 | 
02-19 11:13:08 [LP epoch 25] train_loss=1.2582 train_acc=0.3866 | val_loss=6121.0397 val_acc=0.3776 | 
02-19 11:13:22 [LP epoch 26] train_loss=1.2557 train_acc=0.3885 | val_loss=6097.0396 val_acc=0.3791 | 
02-19 11:13:36 [LP epoch 27] train_loss=1.2538 train_acc=0.3878 | val_loss=6094.6428 val_acc=0.3801 | 
02-19 11:13:50 [LP epoch 28] train_loss=1.2525 train_acc=0.3903 | val_loss=6058.4204 val_acc=0.4127 | 
02-19 11:14:05 [LP epoch 29] train_loss=1.2533 train_acc=0.3889 | val_loss=6117.7012 val_acc=0.3760 | 
02-19 11:14:19 [LP epoch 30] train_loss=1.2566 train_acc=0.3847 | val_loss=6125.3808 val_acc=0.3747 | 
02-19 11:14:34 [LP epoch 31] train_loss=1.2559 train_acc=0.3854 | val_loss=6118.7935 val_acc=0.4096 | 
02-19 11:14:48 [LP epoch 32] train_loss=1.2546 train_acc=0.3847 | val_loss=6121.6256 val_acc=0.4011 | 
02-19 11:15:02 [LP epoch 33] train_loss=1.2536 train_acc=0.3879 | val_loss=6113.6545 val_acc=0.3986 | 
02-19 11:15:16 [LP epoch 34] train_loss=1.2564 train_acc=0.3846 | val_loss=6117.9127 val_acc=0.3754 | 
02-19 11:15:30 [LP epoch 35] train_loss=1.2549 train_acc=0.3852 | val_loss=6112.5069 val_acc=0.3758 | 
02-19 11:15:45 [LP epoch 36] train_loss=1.2534 train_acc=0.3867 | val_loss=6115.0067 val_acc=0.3818 | 
02-19 11:15:59 [LP epoch 37] train_loss=1.2521 train_acc=0.3904 | val_loss=6083.3351 val_acc=0.4137 | 
02-19 11:16:14 [LP epoch 38] train_loss=1.2528 train_acc=0.3871 | val_loss=6187.6095 val_acc=0.3745 | 
02-19 11:16:28 [LP epoch 39] train_loss=1.2548 train_acc=0.3867 | val_loss=6110.0781 val_acc=0.3739 | 
02-19 11:16:42 [LP epoch 40] train_loss=1.2524 train_acc=0.3854 | val_loss=6085.9242 val_acc=0.3781 | 
02-19 11:16:57 [LP epoch 41] train_loss=1.2526 train_acc=0.3856 | val_loss=6077.0088 val_acc=0.4148 | 
02-19 11:17:10 [LP epoch 42] train_loss=1.2520 train_acc=0.3876 | val_loss=6133.3873 val_acc=0.4005 | 
02-19 11:17:25 [LP epoch 43] train_loss=1.2517 train_acc=0.3881 | val_loss=6117.6604 val_acc=0.4013 | 
02-19 11:17:39 [LP epoch 44] train_loss=1.2525 train_acc=0.3939 | val_loss=6118.3052 val_acc=0.3739 | 
02-19 11:17:53 [LP epoch 45] train_loss=1.2506 train_acc=0.3885 | val_loss=6129.9139 val_acc=0.3747 | 
02-19 11:18:07 [LP epoch 46] train_loss=1.2498 train_acc=0.3928 | val_loss=6079.5941 val_acc=0.4104 | 
02-19 11:18:21 [LP epoch 47] train_loss=1.2527 train_acc=0.3902 | val_loss=6113.8563 val_acc=0.3768 | 
02-19 11:18:36 [LP epoch 48] train_loss=1.2532 train_acc=0.3865 | val_loss=6126.2177 val_acc=0.4036 | 
02-19 11:18:49 [LP epoch 49] train_loss=1.2535 train_acc=0.3881 | val_loss=6138.4240 val_acc=0.3747 | 
02-19 11:19:03 [LP epoch 50] train_loss=1.2532 train_acc=0.3886 | val_loss=6098.5579 val_acc=0.3789 | 
02-19 11:19:17 [LP epoch 51] train_loss=1.2526 train_acc=0.3891 | val_loss=6106.1582 val_acc=0.3787 | 
02-19 11:19:31 [LP epoch 52] train_loss=1.2547 train_acc=0.3896 | val_loss=6102.3545 val_acc=0.3791 | 
02-19 11:19:45 [LP epoch 53] train_loss=1.2529 train_acc=0.3894 | val_loss=6091.6495 val_acc=0.3768 | 
02-19 11:19:59 [LP epoch 54] train_loss=1.2517 train_acc=0.3913 | val_loss=6111.3415 val_acc=0.3793 | 
02-19 11:20:13 [LP epoch 55] train_loss=1.2561 train_acc=0.3910 | val_loss=6069.7345 val_acc=0.4050 | 
02-19 11:20:27 [LP epoch 56] train_loss=1.2487 train_acc=0.3953 | val_loss=6034.0937 val_acc=0.4185 | 
02-19 11:20:41 [LP epoch 57] train_loss=1.2555 train_acc=0.3865 | val_loss=6084.5044 val_acc=0.3797 | 
02-19 11:20:55 [LP epoch 58] train_loss=1.2509 train_acc=0.3912 | val_loss=6443.8431 val_acc=0.3984 | 
02-19 11:21:09 [LP epoch 59] train_loss=1.2480 train_acc=0.3935 | val_loss=6102.5361 val_acc=0.3959 | 
02-19 11:21:09 Saved linear-probe checkpoint: ./checkpoint\SimSiamResNet_PU_0219-105601\linear_probe_best.pt (best_val_acc=0.4185)
02-19 11:21:10 TEST linear-probe: loss=1.2668 acc=0.3948
02-19 11:21:10 using 1 gpus
02-19 11:21:10 Dataset class: <class 'data_utils.datasets.PU.PU'>
02-19 11:21:11 Split sizes: train=22502 val=4822 test=4823
02-19 11:21:12 Label counts train: Counter({1: 8481, 3: 7718, 0: 4201, 2: 2102})
02-19 11:21:12 Label counts val:   Counter({1: 1817, 3: 1654, 0: 900, 2: 451})
02-19 11:21:13 Label counts test:  Counter({1: 1818, 3: 1654, 0: 901, 2: 450})
02-19 11:21:15 -----Epoch 0/19-----
02-19 11:21:45 current lr: 0.01
02-19 11:21:45 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9865)
02-19 11:21:45 Epoch 000 Train loss -0.9300 | Val loss -0.9865
02-19 11:21:45 -----Epoch 1/19-----
02-19 11:22:14 current lr: 0.0099385
02-19 11:22:14 Epoch 001 Train loss -0.9674 | Val loss -0.9649
02-19 11:22:14 -----Epoch 2/19-----
02-19 11:22:44 current lr: 0.00975553
02-19 11:22:44 Epoch 002 Train loss -0.9730 | Val loss -0.9841
02-19 11:22:44 -----Epoch 3/19-----
02-19 11:23:15 current lr: 0.00945558
02-19 11:23:15 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9869)
02-19 11:23:15 Epoch 003 Train loss -0.9896 | Val loss -0.9869
02-19 11:23:15 -----Epoch 4/19-----
02-19 11:23:46 current lr: 0.00904604
02-19 11:23:46 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9879)
02-19 11:23:46 Epoch 004 Train loss -0.9926 | Val loss -0.9879
02-19 11:23:46 -----Epoch 5/19-----
02-19 11:24:17 current lr: 0.008537
02-19 11:24:17 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9928)
02-19 11:24:17 Epoch 005 Train loss -0.9938 | Val loss -0.9928
02-19 11:24:17 -----Epoch 6/19-----
02-19 11:24:46 current lr: 0.00794099
02-19 11:24:46 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9944)
02-19 11:24:46 Epoch 006 Train loss -0.9932 | Val loss -0.9944
02-19 11:24:46 -----Epoch 7/19-----
02-19 11:25:12 current lr: 0.00727268
02-19 11:25:12 Epoch 007 Train loss -0.9954 | Val loss -0.9938
02-19 11:25:12 -----Epoch 8/19-----
02-19 11:25:39 current lr: 0.00654854
02-19 11:25:39 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9957)
02-19 11:25:39 Epoch 008 Train loss -0.9959 | Val loss -0.9957
02-19 11:25:39 -----Epoch 9/19-----
02-19 11:26:07 current lr: 0.00578639
02-19 11:26:07 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9966)
02-19 11:26:07 Epoch 009 Train loss -0.9963 | Val loss -0.9966
02-19 11:26:07 -----Epoch 10/19-----
02-19 11:26:34 current lr: 0.005005
02-19 11:26:34 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9966)
02-19 11:26:34 Epoch 010 Train loss -0.9966 | Val loss -0.9966
02-19 11:26:34 -----Epoch 11/19-----
02-19 11:27:03 current lr: 0.00422361
02-19 11:27:03 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9970)
02-19 11:27:03 Epoch 011 Train loss -0.9969 | Val loss -0.9970
02-19 11:27:03 -----Epoch 12/19-----
02-19 11:27:30 current lr: 0.00346146
02-19 11:27:30 Epoch 012 Train loss -0.9969 | Val loss -0.9969
02-19 11:27:30 -----Epoch 13/19-----
02-19 11:27:58 current lr: 0.00273732
02-19 11:27:58 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9970)
02-19 11:27:58 Epoch 013 Train loss -0.9970 | Val loss -0.9970
02-19 11:27:58 -----Epoch 14/19-----
02-19 11:28:26 current lr: 0.00206901
02-19 11:28:27 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9972)
02-19 11:28:27 Epoch 014 Train loss -0.9971 | Val loss -0.9972
02-19 11:28:27 -----Epoch 15/19-----
02-19 11:28:53 current lr: 0.001473
02-19 11:28:53 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9972)
02-19 11:28:53 Epoch 015 Train loss -0.9970 | Val loss -0.9972
02-19 11:28:53 -----Epoch 16/19-----
02-19 11:29:24 current lr: 0.00096396
02-19 11:29:24 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9973)
02-19 11:29:24 Epoch 016 Train loss -0.9972 | Val loss -0.9973
02-19 11:29:24 -----Epoch 17/19-----
02-19 11:29:51 current lr: 0.000554422
02-19 11:29:51 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9973)
02-19 11:29:51 Epoch 017 Train loss -0.9972 | Val loss -0.9973
02-19 11:29:51 -----Epoch 18/19-----
02-19 11:30:19 current lr: 0.000254473
02-19 11:30:19 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9973)
02-19 11:30:19 Epoch 018 Train loss -0.9972 | Val loss -0.9973
02-19 11:30:19 -----Epoch 19/19-----
02-19 11:30:48 current lr: 7.14967e-05
02-19 11:30:48 Epoch 019 Train loss -0.9972 | Val loss -0.9973
02-19 11:30:49 TEST (last): loss -0.9972
02-19 11:31:04 [LP epoch 00] train_loss=1.2451 train_acc=0.4035 | val_loss=5883.5836 val_acc=0.4438 | 
02-19 11:31:18 [LP epoch 01] train_loss=1.2253 train_acc=0.4212 | val_loss=5837.3982 val_acc=0.4432 | 
02-19 11:31:33 [LP epoch 02] train_loss=1.2112 train_acc=0.4294 | val_loss=6461.6344 val_acc=0.4359 | 
02-19 11:31:47 [LP epoch 03] train_loss=1.2127 train_acc=0.4274 | val_loss=5806.0650 val_acc=0.4428 | 
02-19 11:32:01 [LP epoch 04] train_loss=1.2029 train_acc=0.4338 | val_loss=5848.5302 val_acc=0.4453 | 
02-19 11:32:15 [LP epoch 05] train_loss=1.2009 train_acc=0.4376 | val_loss=5854.7295 val_acc=0.4467 | 
02-19 11:32:29 [LP epoch 06] train_loss=1.1985 train_acc=0.4336 | val_loss=5705.3067 val_acc=0.4681 | 
02-19 11:32:44 [LP epoch 07] train_loss=1.1933 train_acc=0.4393 | val_loss=5742.6914 val_acc=0.4643 | 
02-19 11:32:58 [LP epoch 08] train_loss=1.1905 train_acc=0.4424 | val_loss=5717.5462 val_acc=0.4706 | 
02-19 11:33:12 [LP epoch 09] train_loss=1.1879 train_acc=0.4433 | val_loss=5659.8890 val_acc=0.4732 | 
02-19 11:33:26 [LP epoch 10] train_loss=1.1879 train_acc=0.4458 | val_loss=5633.3805 val_acc=0.4803 | 
02-19 11:33:40 [LP epoch 11] train_loss=1.1857 train_acc=0.4485 | val_loss=5604.7609 val_acc=0.4857 | 
02-19 11:33:54 [LP epoch 12] train_loss=1.1856 train_acc=0.4436 | val_loss=5655.9118 val_acc=0.4776 | 
02-19 11:34:08 [LP epoch 13] train_loss=1.1805 train_acc=0.4500 | val_loss=6018.3516 val_acc=0.4652 | 
02-19 11:34:22 [LP epoch 14] train_loss=1.1776 train_acc=0.4528 | val_loss=5609.9875 val_acc=0.4757 | 
02-19 11:34:36 [LP epoch 15] train_loss=1.1792 train_acc=0.4534 | val_loss=5610.8674 val_acc=0.4838 | 
02-19 11:34:50 [LP epoch 16] train_loss=1.1795 train_acc=0.4535 | val_loss=5584.7038 val_acc=0.4913 | 
02-19 11:35:04 [LP epoch 17] train_loss=1.1774 train_acc=0.4544 | val_loss=5647.4809 val_acc=0.4693 | 
02-19 11:35:18 [LP epoch 18] train_loss=1.1727 train_acc=0.4557 | val_loss=5739.8568 val_acc=0.4807 | 
02-19 11:35:32 [LP epoch 19] train_loss=1.1771 train_acc=0.4540 | val_loss=5679.8726 val_acc=0.4822 | 
02-19 11:35:46 [LP epoch 20] train_loss=1.1711 train_acc=0.4583 | val_loss=5561.0215 val_acc=0.4894 | 
02-19 11:36:00 [LP epoch 21] train_loss=1.1708 train_acc=0.4573 | val_loss=5677.6251 val_acc=0.4815 | 
02-19 11:36:14 [LP epoch 22] train_loss=1.1730 train_acc=0.4587 | val_loss=5592.8635 val_acc=0.4894 | 
02-19 11:36:29 [LP epoch 23] train_loss=1.1739 train_acc=0.4576 | val_loss=5594.9022 val_acc=0.4853 | 
02-19 11:36:43 [LP epoch 24] train_loss=1.1686 train_acc=0.4599 | val_loss=5552.7760 val_acc=0.4888 | 
02-19 11:36:57 [LP epoch 25] train_loss=1.1670 train_acc=0.4604 | val_loss=5532.8606 val_acc=0.4934 | 
02-19 11:37:11 [LP epoch 26] train_loss=1.1686 train_acc=0.4629 | val_loss=5551.4860 val_acc=0.4950 | 
02-19 11:37:25 [LP epoch 27] train_loss=1.1670 train_acc=0.4580 | val_loss=5542.5614 val_acc=0.4944 | 
02-19 11:37:39 [LP epoch 28] train_loss=1.1682 train_acc=0.4628 | val_loss=5569.5890 val_acc=0.4849 | 
02-19 11:37:53 [LP epoch 29] train_loss=1.1689 train_acc=0.4608 | val_loss=5625.5268 val_acc=0.4909 | 
02-19 11:38:07 [LP epoch 30] train_loss=1.1690 train_acc=0.4580 | val_loss=5576.5054 val_acc=0.4871 | 
02-19 11:38:21 [LP epoch 31] train_loss=1.1676 train_acc=0.4615 | val_loss=5551.9844 val_acc=0.4936 | 
02-19 11:38:35 [LP epoch 32] train_loss=1.1668 train_acc=0.4618 | val_loss=5563.9184 val_acc=0.4940 | 
02-19 11:38:50 [LP epoch 33] train_loss=1.1640 train_acc=0.4620 | val_loss=5540.2667 val_acc=0.4903 | 
02-19 11:39:04 [LP epoch 34] train_loss=1.1661 train_acc=0.4619 | val_loss=5544.4228 val_acc=0.4919 | 
02-19 11:39:18 [LP epoch 35] train_loss=1.1713 train_acc=0.4600 | val_loss=5623.0162 val_acc=0.4724 | 
02-19 11:39:32 [LP epoch 36] train_loss=1.1682 train_acc=0.4613 | val_loss=5542.6814 val_acc=0.4907 | 
02-19 11:39:46 [LP epoch 37] train_loss=1.1696 train_acc=0.4627 | val_loss=5601.4152 val_acc=0.4936 | 
02-19 11:40:00 [LP epoch 38] train_loss=1.1638 train_acc=0.4636 | val_loss=5672.2443 val_acc=0.4921 | 
02-19 11:40:14 [LP epoch 39] train_loss=1.1638 train_acc=0.4631 | val_loss=5542.6827 val_acc=0.4925 | 
02-19 11:40:28 [LP epoch 40] train_loss=1.1636 train_acc=0.4624 | val_loss=5565.1127 val_acc=0.4969 | 
02-19 11:40:43 [LP epoch 41] train_loss=1.1630 train_acc=0.4643 | val_loss=5650.6134 val_acc=0.4923 | 
02-19 11:40:57 [LP epoch 42] train_loss=1.1629 train_acc=0.4645 | val_loss=5517.1730 val_acc=0.4929 | 
02-19 11:41:11 [LP epoch 43] train_loss=1.1616 train_acc=0.4656 | val_loss=5519.9704 val_acc=0.4967 | 
02-19 11:41:25 [LP epoch 44] train_loss=1.1620 train_acc=0.4618 | val_loss=5528.1799 val_acc=0.4959 | 
02-19 11:41:40 [LP epoch 45] train_loss=1.1610 train_acc=0.4637 | val_loss=5521.8584 val_acc=0.4950 | 
02-19 11:41:54 [LP epoch 46] train_loss=1.1616 train_acc=0.4655 | val_loss=5518.1108 val_acc=0.4956 | 
02-19 11:42:08 [LP epoch 47] train_loss=1.1599 train_acc=0.4637 | val_loss=5516.3657 val_acc=0.4954 | 
02-19 11:42:23 [LP epoch 48] train_loss=1.1663 train_acc=0.4638 | val_loss=5581.2176 val_acc=0.4857 | 
02-19 11:42:37 [LP epoch 49] train_loss=1.1628 train_acc=0.4665 | val_loss=5551.1861 val_acc=0.4896 | 
02-19 11:42:52 [LP epoch 50] train_loss=1.1603 train_acc=0.4663 | val_loss=5521.5397 val_acc=0.4913 | 
02-19 11:43:06 [LP epoch 51] train_loss=1.1604 train_acc=0.4651 | val_loss=5520.9887 val_acc=0.4944 | 
02-19 11:43:20 [LP epoch 52] train_loss=1.1591 train_acc=0.4664 | val_loss=5508.7853 val_acc=0.4973 | 
02-19 11:43:34 [LP epoch 53] train_loss=1.1604 train_acc=0.4665 | val_loss=5540.8438 val_acc=0.4890 | 
02-19 11:43:49 [LP epoch 54] train_loss=1.1607 train_acc=0.4656 | val_loss=5519.7385 val_acc=0.4985 | 
02-19 11:44:03 [LP epoch 55] train_loss=1.1596 train_acc=0.4651 | val_loss=5506.3188 val_acc=0.4925 | 
02-19 11:44:17 [LP epoch 56] train_loss=1.1592 train_acc=0.4656 | val_loss=5525.6013 val_acc=0.4932 | 
02-19 11:44:32 [LP epoch 57] train_loss=1.1600 train_acc=0.4633 | val_loss=5508.0704 val_acc=0.4959 | 
02-19 11:44:46 [LP epoch 58] train_loss=1.1581 train_acc=0.4664 | val_loss=5510.8761 val_acc=0.4950 | 
02-19 11:45:00 [LP epoch 59] train_loss=1.1590 train_acc=0.4684 | val_loss=5509.2323 val_acc=0.4959 | 
02-19 11:45:00 Saved linear-probe checkpoint: ./checkpoint\SimSiamResNet_PU_0219-105601\linear_probe_best.pt (best_val_acc=0.4985)
02-19 11:45:02 TEST linear-probe: loss=1.1427 acc=0.4810
02-19 11:45:02 using 1 gpus
02-19 11:45:02 Dataset class: <class 'data_utils.datasets.PU.PU'>
02-19 11:45:02 Split sizes: train=22502 val=4822 test=4823
02-19 11:45:04 Label counts train: Counter({1: 8481, 3: 7718, 0: 4201, 2: 2102})
02-19 11:45:04 Label counts val:   Counter({1: 1817, 3: 1654, 0: 900, 2: 451})
02-19 11:45:04 Label counts test:  Counter({1: 1818, 3: 1654, 0: 901, 2: 450})
02-19 11:45:07 -----Epoch 0/19-----
02-19 11:45:35 current lr: 0.01
02-19 11:45:35 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9859)
02-19 11:45:35 Epoch 000 Train loss -0.9209 | Val loss -0.9859
02-19 11:45:35 -----Epoch 1/19-----
02-19 11:46:03 current lr: 0.0099385
02-19 11:46:03 Epoch 001 Train loss -0.9859 | Val loss -0.9797
02-19 11:46:03 -----Epoch 2/19-----
02-19 11:46:29 current lr: 0.00975553
02-19 11:46:30 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9942)
02-19 11:46:30 Epoch 002 Train loss -0.9891 | Val loss -0.9942
02-19 11:46:30 -----Epoch 3/19-----
02-19 11:46:58 current lr: 0.00945558
02-19 11:46:58 Epoch 003 Train loss -0.9937 | Val loss -0.9913
02-19 11:46:58 -----Epoch 4/19-----
02-19 11:47:24 current lr: 0.00904604
02-19 11:47:24 Epoch 004 Train loss -0.9911 | Val loss -0.9896
02-19 11:47:24 -----Epoch 5/19-----
02-19 11:47:54 current lr: 0.008537
02-19 11:47:54 Epoch 005 Train loss -0.9926 | Val loss -0.9901
02-19 11:47:54 -----Epoch 6/19-----
02-19 11:48:21 current lr: 0.00794099
02-19 11:48:21 Epoch 006 Train loss -0.9938 | Val loss -0.9918
02-19 11:48:21 -----Epoch 7/19-----
02-19 11:48:48 current lr: 0.00727268
02-19 11:48:48 Epoch 007 Train loss -0.9947 | Val loss -0.9921
02-19 11:48:48 -----Epoch 8/19-----
02-19 11:49:17 current lr: 0.00654854
02-19 11:49:17 Epoch 008 Train loss -0.9948 | Val loss -0.9928
02-19 11:49:17 -----Epoch 9/19-----
02-19 11:49:43 current lr: 0.00578639
02-19 11:49:43 Epoch 009 Train loss -0.9923 | Val loss -0.9764
02-19 11:49:43 -----Epoch 10/19-----
02-19 11:50:13 current lr: 0.005005
02-19 11:50:13 Epoch 010 Train loss -0.9913 | Val loss -0.9921
02-19 11:50:13 -----Epoch 11/19-----
02-19 11:50:41 current lr: 0.00422361
02-19 11:50:41 Epoch 011 Train loss -0.9937 | Val loss -0.9918
02-19 11:50:41 -----Epoch 12/19-----
02-19 11:51:09 current lr: 0.00346146
02-19 11:51:09 Epoch 012 Train loss -0.9954 | Val loss -0.9938
02-19 11:51:09 -----Epoch 13/19-----
02-19 11:51:38 current lr: 0.00273732
02-19 11:51:38 Epoch 013 Train loss -0.9965 | Val loss -0.9928
02-19 11:51:38 -----Epoch 14/19-----
02-19 11:52:04 current lr: 0.00206901
02-19 11:52:04 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9953)
02-19 11:52:04 Epoch 014 Train loss -0.9972 | Val loss -0.9953
02-19 11:52:04 -----Epoch 15/19-----
02-19 11:52:33 current lr: 0.001473
02-19 11:52:33 Epoch 015 Train loss -0.9976 | Val loss -0.9950
02-19 11:52:33 -----Epoch 16/19-----
02-19 11:53:01 current lr: 0.00096396
02-19 11:53:01 Epoch 016 Train loss -0.9976 | Val loss -0.9949
02-19 11:53:01 -----Epoch 17/19-----
02-19 11:53:28 current lr: 0.000554422
02-19 11:53:28 Epoch 017 Train loss -0.9975 | Val loss -0.9950
02-19 11:53:28 -----Epoch 18/19-----
02-19 11:53:58 current lr: 0.000254473
02-19 11:53:58 Epoch 018 Train loss -0.9976 | Val loss -0.9945
02-19 11:53:58 -----Epoch 19/19-----
02-19 11:54:24 current lr: 7.14967e-05
02-19 11:54:24 Epoch 019 Train loss -0.9976 | Val loss -0.9948
02-19 11:54:25 TEST (last): loss -0.9945
02-19 11:54:40 [LP epoch 00] train_loss=1.2761 train_acc=0.3673 | val_loss=6130.4271 val_acc=0.3725 | 
02-19 11:54:54 [LP epoch 01] train_loss=1.2635 train_acc=0.3788 | val_loss=6131.1900 val_acc=0.3764 | 
02-19 11:55:09 [LP epoch 02] train_loss=1.2606 train_acc=0.3838 | val_loss=6119.3237 val_acc=0.3760 | 
02-19 11:55:23 [LP epoch 03] train_loss=1.2598 train_acc=0.3825 | val_loss=6124.9002 val_acc=0.3774 | 
02-19 11:55:38 [LP epoch 04] train_loss=1.2578 train_acc=0.3840 | val_loss=6217.2742 val_acc=0.3716 | 
02-19 11:55:53 [LP epoch 05] train_loss=1.2594 train_acc=0.3871 | val_loss=6138.4232 val_acc=0.3754 | 
02-19 11:56:07 [LP epoch 06] train_loss=1.2590 train_acc=0.3854 | val_loss=6123.1350 val_acc=0.3749 | 
02-19 11:56:21 [LP epoch 07] train_loss=1.2578 train_acc=0.3842 | val_loss=6122.0692 val_acc=0.3772 | 
02-19 11:56:36 [LP epoch 08] train_loss=1.2577 train_acc=0.3822 | val_loss=6140.7189 val_acc=0.3760 | 
02-19 11:56:51 [LP epoch 09] train_loss=1.2576 train_acc=0.3843 | val_loss=6127.7005 val_acc=0.3770 | 
02-19 11:57:05 [LP epoch 10] train_loss=1.2564 train_acc=0.3885 | val_loss=6130.2777 val_acc=0.3770 | 
02-19 11:57:20 [LP epoch 11] train_loss=1.2568 train_acc=0.3868 | val_loss=6123.8712 val_acc=0.3758 | 
02-19 11:57:34 [LP epoch 12] train_loss=1.2558 train_acc=0.3873 | val_loss=6124.3666 val_acc=0.3762 | 
02-19 11:57:49 [LP epoch 13] train_loss=1.2555 train_acc=0.3868 | val_loss=6140.9275 val_acc=0.3733 | 
02-19 11:58:03 [LP epoch 14] train_loss=1.2555 train_acc=0.3878 | val_loss=6117.8857 val_acc=0.3766 | 
02-19 11:58:17 [LP epoch 15] train_loss=1.2548 train_acc=0.3879 | val_loss=6157.5795 val_acc=0.4032 | 
02-19 11:58:31 [LP epoch 16] train_loss=1.2558 train_acc=0.3880 | val_loss=6126.1360 val_acc=0.3783 | 
02-19 11:58:45 [LP epoch 17] train_loss=1.2560 train_acc=0.3833 | val_loss=6123.8523 val_acc=0.3783 | 
02-19 11:59:00 [LP epoch 18] train_loss=1.2556 train_acc=0.3857 | val_loss=6122.3940 val_acc=0.3781 | 
02-19 11:59:14 [LP epoch 19] train_loss=1.2550 train_acc=0.3860 | val_loss=6116.0786 val_acc=0.3772 | 
02-19 11:59:29 [LP epoch 20] train_loss=1.2542 train_acc=0.3879 | val_loss=6120.1319 val_acc=0.3779 | 
02-19 11:59:43 [LP epoch 21] train_loss=1.2545 train_acc=0.3875 | val_loss=6121.5837 val_acc=0.3770 | 
02-19 11:59:57 [LP epoch 22] train_loss=1.2548 train_acc=0.3869 | val_loss=6123.8694 val_acc=0.3781 | 
02-19 12:00:12 [LP epoch 23] train_loss=1.2558 train_acc=0.3874 | val_loss=6118.9919 val_acc=0.3789 | 
02-19 12:00:26 [LP epoch 24] train_loss=1.2545 train_acc=0.3904 | val_loss=6144.6934 val_acc=0.3745 | 
02-19 12:00:40 [LP epoch 25] train_loss=1.2544 train_acc=0.3894 | val_loss=6117.5071 val_acc=0.3766 | 
02-19 12:00:54 [LP epoch 26] train_loss=1.2536 train_acc=0.3889 | val_loss=6096.6666 val_acc=0.3776 | 
02-19 12:01:09 [LP epoch 27] train_loss=1.2529 train_acc=0.3910 | val_loss=6070.1412 val_acc=0.3776 | 
02-19 12:01:24 [LP epoch 28] train_loss=1.2548 train_acc=0.3873 | val_loss=6117.2179 val_acc=0.3772 | 
02-19 12:01:38 [LP epoch 29] train_loss=1.2539 train_acc=0.3909 | val_loss=6107.7882 val_acc=0.3766 | 
02-19 12:01:52 [LP epoch 30] train_loss=1.2514 train_acc=0.3933 | val_loss=6096.2024 val_acc=0.3893 | 
02-19 12:02:06 [LP epoch 31] train_loss=1.2533 train_acc=0.3897 | val_loss=6103.9993 val_acc=0.3762 | 
02-19 12:02:20 [LP epoch 32] train_loss=1.2524 train_acc=0.3926 | val_loss=6076.9708 val_acc=0.3768 | 
02-19 12:02:34 [LP epoch 33] train_loss=1.2509 train_acc=0.3964 | val_loss=6091.4686 val_acc=0.3776 | 
02-19 12:02:49 [LP epoch 34] train_loss=1.2550 train_acc=0.3869 | val_loss=6110.0893 val_acc=0.3779 | 
02-19 12:03:03 [LP epoch 35] train_loss=1.2520 train_acc=0.3905 | val_loss=6086.6398 val_acc=0.3826 | 
02-19 12:03:18 [LP epoch 36] train_loss=1.2499 train_acc=0.3951 | val_loss=6096.8160 val_acc=0.3781 | 
02-19 12:03:32 [LP epoch 37] train_loss=1.2519 train_acc=0.3935 | val_loss=6116.1276 val_acc=0.3752 | 
02-19 12:03:47 [LP epoch 38] train_loss=1.2525 train_acc=0.3921 | val_loss=6114.2328 val_acc=0.3781 | 
02-19 12:04:02 [LP epoch 39] train_loss=1.2526 train_acc=0.3921 | val_loss=6101.6918 val_acc=0.3789 | 
02-19 12:04:16 [LP epoch 40] train_loss=1.2554 train_acc=0.3908 | val_loss=6109.7312 val_acc=0.3758 | 
02-19 12:04:31 [LP epoch 41] train_loss=1.2522 train_acc=0.3975 | val_loss=6096.6625 val_acc=0.3787 | 
02-19 12:04:45 [LP epoch 42] train_loss=1.2519 train_acc=0.3924 | val_loss=6106.3462 val_acc=0.3783 | 
02-19 12:05:00 [LP epoch 43] train_loss=1.2507 train_acc=0.3955 | val_loss=6099.5580 val_acc=0.3781 | 
02-19 12:05:14 [LP epoch 44] train_loss=1.2535 train_acc=0.3942 | val_loss=6108.2340 val_acc=0.3781 | 
02-19 12:05:29 [LP epoch 45] train_loss=1.2517 train_acc=0.3977 | val_loss=6109.3628 val_acc=0.3766 | 
02-19 12:05:43 [LP epoch 46] train_loss=1.2532 train_acc=0.3906 | val_loss=6108.0065 val_acc=0.3789 | 
02-19 12:05:58 [LP epoch 47] train_loss=1.2529 train_acc=0.3926 | val_loss=6114.2709 val_acc=0.3787 | 
02-19 12:06:12 [LP epoch 48] train_loss=1.2500 train_acc=0.3951 | val_loss=6089.6753 val_acc=0.3783 | 
02-19 12:06:27 [LP epoch 49] train_loss=1.2515 train_acc=0.3958 | val_loss=6113.0131 val_acc=0.3766 | 
02-19 12:06:41 [LP epoch 50] train_loss=1.2519 train_acc=0.3939 | val_loss=6111.5584 val_acc=0.3764 | 
02-19 12:06:56 [LP epoch 51] train_loss=1.2506 train_acc=0.3946 | val_loss=6098.6952 val_acc=0.3791 | 
02-19 12:07:10 [LP epoch 52] train_loss=1.2504 train_acc=0.3961 | val_loss=6106.8410 val_acc=0.3781 | 
02-19 12:07:25 [LP epoch 53] train_loss=1.2513 train_acc=0.3936 | val_loss=6113.8409 val_acc=0.3783 | 
02-19 12:07:39 [LP epoch 54] train_loss=1.2517 train_acc=0.3924 | val_loss=6110.6584 val_acc=0.3789 | 
02-19 12:07:54 [LP epoch 55] train_loss=1.2489 train_acc=0.3955 | val_loss=6107.2742 val_acc=0.3795 | 
02-19 12:08:09 [LP epoch 56] train_loss=1.2511 train_acc=0.3966 | val_loss=6098.1953 val_acc=0.3789 | 
02-19 12:08:23 [LP epoch 57] train_loss=1.2493 train_acc=0.3966 | val_loss=6097.3092 val_acc=0.3783 | 
02-19 12:08:38 [LP epoch 58] train_loss=1.2495 train_acc=0.3979 | val_loss=6112.0739 val_acc=0.3772 | 
02-19 12:08:52 [LP epoch 59] train_loss=1.2506 train_acc=0.3890 | val_loss=6078.4782 val_acc=0.3783 | 
02-19 12:08:52 Saved linear-probe checkpoint: ./checkpoint\SimSiamResNet_PU_0219-105601\linear_probe_best.pt (best_val_acc=0.4032)
02-19 12:08:54 TEST linear-probe: loss=1.2606 acc=0.3796
02-19 12:08:54 using 1 gpus
02-19 12:08:54 Dataset class: <class 'data_utils.datasets.PU.PU'>
02-19 12:08:54 Split sizes: train=22502 val=4822 test=4823
02-19 12:08:56 Label counts train: Counter({1: 8481, 3: 7718, 0: 4201, 2: 2102})
02-19 12:08:56 Label counts val:   Counter({1: 1817, 3: 1654, 0: 900, 2: 451})
02-19 12:08:56 Label counts test:  Counter({1: 1818, 3: 1654, 0: 901, 2: 450})
02-19 12:08:59 -----Epoch 0/19-----
02-19 12:09:28 current lr: 0.01
02-19 12:09:28 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9798)
02-19 12:09:28 Epoch 000 Train loss -0.9387 | Val loss -0.9798
02-19 12:09:28 -----Epoch 1/19-----
02-19 12:09:54 current lr: 0.0099385
02-19 12:09:54 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9832)
02-19 12:09:54 Epoch 001 Train loss -0.9845 | Val loss -0.9832
02-19 12:09:54 -----Epoch 2/19-----
02-19 12:10:23 current lr: 0.00975553
02-19 12:10:23 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9863)
02-19 12:10:23 Epoch 002 Train loss -0.9836 | Val loss -0.9863
02-19 12:10:23 -----Epoch 3/19-----
02-19 12:10:50 current lr: 0.00945558
02-19 12:10:50 Epoch 003 Train loss -0.9910 | Val loss -0.9856
02-19 12:10:50 -----Epoch 4/19-----
02-19 12:11:16 current lr: 0.00904604
02-19 12:11:16 Epoch 004 Train loss -0.9920 | Val loss -0.9833
02-19 12:11:16 -----Epoch 5/19-----
02-19 12:11:46 current lr: 0.008537
02-19 12:11:46 Epoch 005 Train loss -0.9925 | Val loss -0.9814
02-19 12:11:46 -----Epoch 6/19-----
02-19 12:12:12 current lr: 0.00794099
02-19 12:12:12 Epoch 006 Train loss -0.9938 | Val loss -0.9851
02-19 12:12:12 -----Epoch 7/19-----
02-19 12:12:42 current lr: 0.00727268
02-19 12:12:42 Epoch 007 Train loss -0.9962 | Val loss -0.9856
02-19 12:12:42 -----Epoch 8/19-----
02-19 12:13:09 current lr: 0.00654854
02-19 12:13:09 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9870)
02-19 12:13:09 Epoch 008 Train loss -0.9941 | Val loss -0.9870
02-19 12:13:09 -----Epoch 9/19-----
02-19 12:13:37 current lr: 0.00578639
02-19 12:13:37 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9947)
02-19 12:13:37 Epoch 009 Train loss -0.9933 | Val loss -0.9947
02-19 12:13:37 -----Epoch 10/19-----
02-19 12:14:05 current lr: 0.005005
02-19 12:14:05 Epoch 010 Train loss -0.9948 | Val loss -0.9872
02-19 12:14:05 -----Epoch 11/19-----
02-19 12:14:32 current lr: 0.00422361
02-19 12:14:32 Epoch 011 Train loss -0.9957 | Val loss -0.9885
02-19 12:14:32 -----Epoch 12/19-----
02-19 12:15:03 current lr: 0.00346146
02-19 12:15:03 Epoch 012 Train loss -0.9965 | Val loss -0.9887
02-19 12:15:03 -----Epoch 13/19-----
02-19 12:15:29 current lr: 0.00273732
02-19 12:15:29 Epoch 013 Train loss -0.9967 | Val loss -0.9891
02-19 12:15:29 -----Epoch 14/19-----
02-19 12:15:57 current lr: 0.00206901
02-19 12:15:57 Epoch 014 Train loss -0.9968 | Val loss -0.9903
02-19 12:15:57 -----Epoch 15/19-----
02-19 12:16:26 current lr: 0.001473
02-19 12:16:26 Epoch 015 Train loss -0.9969 | Val loss -0.9925
02-19 12:16:26 -----Epoch 16/19-----
02-19 12:16:52 current lr: 0.00096396
02-19 12:16:52 Epoch 016 Train loss -0.9970 | Val loss -0.9926
02-19 12:16:52 -----Epoch 17/19-----
02-19 12:17:21 current lr: 0.000554422
02-19 12:17:21 Saved best checkpoint to ./checkpoint\SimSiamResNet_PU_0219-105601\best_pt (val_loss=-0.9949)
02-19 12:17:21 Epoch 017 Train loss -0.9970 | Val loss -0.9949
02-19 12:17:21 -----Epoch 18/19-----
02-19 12:17:49 current lr: 0.000254473
02-19 12:17:49 Epoch 018 Train loss -0.9971 | Val loss -0.9944
02-19 12:17:49 -----Epoch 19/19-----
02-19 12:18:16 current lr: 7.14967e-05
02-19 12:18:16 Epoch 019 Train loss -0.9969 | Val loss -0.9943
02-19 12:18:18 TEST (last): loss -0.9942
02-19 12:18:32 [LP epoch 00] train_loss=1.2675 train_acc=0.3816 | val_loss=6111.9395 val_acc=0.3884 | 
02-19 12:18:46 [LP epoch 01] train_loss=1.2586 train_acc=0.3885 | val_loss=6093.6312 val_acc=0.3917 | 
02-19 12:19:01 [LP epoch 02] train_loss=1.2556 train_acc=0.3939 | val_loss=6102.9697 val_acc=0.3872 | 
