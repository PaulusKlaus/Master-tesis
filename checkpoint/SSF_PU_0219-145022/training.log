02-19 14:50:22 model_name: SSF
02-19 14:50:22 data_name: PU
02-19 14:50:22 aug_1: normal
02-19 14:50:22 aug_2: randomcrop
02-19 14:50:22 max_epoch: 20
02-19 14:50:22 classifier_epoch: 50
02-19 14:50:22 data_view: TwoViewDataset
02-19 14:50:22 cuda_device: 0
02-19 14:50:22 checkpoint_dir: ./checkpoint
02-19 14:50:22 batch_size: 32
02-19 14:50:22 data_dir: raw_data/PU
02-19 14:50:22 out_channel: 4
02-19 14:50:22 normlizetype: minus_one_one
02-19 14:50:22 processing_type: RA
02-19 14:50:22 opt: sgd
02-19 14:50:22 lr: 0.01
02-19 14:50:22 momentum: 0.9
02-19 14:50:22 weight_decay: 1e-05
02-19 14:50:22 lr_scheduler: cos
02-19 14:50:22 gamma: 0.1
02-19 14:50:22 eta_min: 1e-05
02-19 14:50:22 task: self_supervised
02-19 14:50:23 critetion: <class 'utils.loss_SSL.SimSiamLoss'>
02-19 14:50:23 using 1 gpus
02-19 14:50:23 Dataset class: <class 'data_utils.datasets.PU.PU'>
02-19 14:50:23 Split sizes: train=22502 val=4822 test=4823
02-19 14:50:25 Label counts train: Counter({1: 8481, 3: 7718, 0: 4201, 2: 2102})
02-19 14:50:25 Label counts val:   Counter({1: 1817, 3: 1654, 0: 900, 2: 451})
02-19 14:50:25 Label counts test:  Counter({1: 1818, 3: 1654, 0: 901, 2: 450})
02-19 14:50:29 -----Epoch 0/19-----
02-19 14:50:48 current lr: 0.01
02-19 14:50:48 Saved best checkpoint to ./checkpoint\SSF_PU_0219-145022\best_pt (val_loss=-0.9856)
02-19 14:50:48 Epoch 000 Train loss -0.9560 | Val loss -0.9856
02-19 14:50:48 -----Epoch 1/19-----
02-19 14:51:05 current lr: 0.0099385
02-19 14:51:05 Saved best checkpoint to ./checkpoint\SSF_PU_0219-145022\best_pt (val_loss=-0.9881)
02-19 14:51:05 Epoch 001 Train loss -0.9855 | Val loss -0.9881
02-19 14:51:05 -----Epoch 2/19-----
02-19 14:51:22 current lr: 0.00975553
02-19 14:51:22 Epoch 002 Train loss -0.9826 | Val loss -0.9847
02-19 14:51:22 -----Epoch 3/19-----
02-19 14:51:40 current lr: 0.00945558
02-19 14:51:40 Saved best checkpoint to ./checkpoint\SSF_PU_0219-145022\best_pt (val_loss=-0.9923)
02-19 14:51:40 Epoch 003 Train loss -0.9899 | Val loss -0.9923
02-19 14:51:40 -----Epoch 4/19-----
02-19 14:51:58 current lr: 0.00904604
02-19 14:51:58 Epoch 004 Train loss -0.9906 | Val loss -0.9910
02-19 14:51:58 -----Epoch 5/19-----
02-19 14:52:16 current lr: 0.008537
02-19 14:52:16 Saved best checkpoint to ./checkpoint\SSF_PU_0219-145022\best_pt (val_loss=-0.9927)
02-19 14:52:16 Epoch 005 Train loss -0.9916 | Val loss -0.9927
02-19 14:52:16 -----Epoch 6/19-----
02-19 14:52:33 current lr: 0.00794099
02-19 14:52:33 Saved best checkpoint to ./checkpoint\SSF_PU_0219-145022\best_pt (val_loss=-0.9952)
02-19 14:52:33 Epoch 006 Train loss -0.9935 | Val loss -0.9952
02-19 14:52:33 -----Epoch 7/19-----
02-19 14:52:50 current lr: 0.00727268
02-19 14:52:51 Saved best checkpoint to ./checkpoint\SSF_PU_0219-145022\best_pt (val_loss=-0.9957)
02-19 14:52:51 Epoch 007 Train loss -0.9948 | Val loss -0.9957
02-19 14:52:51 -----Epoch 8/19-----
02-19 14:53:08 current lr: 0.00654854
02-19 14:53:08 Saved best checkpoint to ./checkpoint\SSF_PU_0219-145022\best_pt (val_loss=-0.9962)
02-19 14:53:08 Epoch 008 Train loss -0.9953 | Val loss -0.9962
02-19 14:53:08 -----Epoch 9/19-----
02-19 14:53:25 current lr: 0.00578639
02-19 14:53:25 Epoch 009 Train loss -0.9955 | Val loss -0.9956
02-19 14:53:25 -----Epoch 10/19-----
02-19 14:53:43 current lr: 0.005005
02-19 14:53:43 Saved best checkpoint to ./checkpoint\SSF_PU_0219-145022\best_pt (val_loss=-0.9967)
02-19 14:53:43 Epoch 010 Train loss -0.9956 | Val loss -0.9967
02-19 14:53:43 -----Epoch 11/19-----
02-19 14:54:00 current lr: 0.00422361
02-19 14:54:00 Epoch 011 Train loss -0.9958 | Val loss -0.9961
02-19 14:54:00 -----Epoch 12/19-----
02-19 14:54:18 current lr: 0.00346146
02-19 14:54:18 Epoch 012 Train loss -0.9940 | Val loss -0.9925
02-19 14:54:18 -----Epoch 13/19-----
02-19 14:54:35 current lr: 0.00273732
02-19 14:54:35 Epoch 013 Train loss -0.9914 | Val loss -0.9929
02-19 14:54:35 -----Epoch 14/19-----
02-19 14:54:53 current lr: 0.00206901
02-19 14:54:53 Epoch 014 Train loss -0.9931 | Val loss -0.9950
02-19 14:54:53 -----Epoch 15/19-----
02-19 14:55:10 current lr: 0.001473
02-19 14:55:10 Epoch 015 Train loss -0.9943 | Val loss -0.9952
02-19 14:55:10 -----Epoch 16/19-----
02-19 14:55:28 current lr: 0.00096396
02-19 14:55:28 Epoch 016 Train loss -0.9944 | Val loss -0.9952
02-19 14:55:28 -----Epoch 17/19-----
02-19 14:55:46 current lr: 0.000554422
02-19 14:55:46 Epoch 017 Train loss -0.9944 | Val loss -0.9952
02-19 14:55:46 -----Epoch 18/19-----
02-19 14:56:03 current lr: 0.000254473
02-19 14:56:03 Epoch 018 Train loss -0.9944 | Val loss -0.9951
02-19 14:56:03 -----Epoch 19/19-----
02-19 14:56:21 current lr: 7.14967e-05
02-19 14:56:21 Epoch 019 Train loss -0.9944 | Val loss -0.9951
02-19 14:56:22 TEST (last): loss -0.9950
02-19 14:56:31 [LP epoch 00] train_loss=1.1654 train_acc=0.4478 | val_loss=5155.0014 val_acc=0.5160 | 
02-19 14:56:40 [LP epoch 01] train_loss=1.0756 train_acc=0.4978 | val_loss=5045.2395 val_acc=0.5241 | 
02-19 14:56:49 [LP epoch 02] train_loss=1.0569 train_acc=0.5048 | val_loss=4955.1829 val_acc=0.5303 | 
02-19 14:56:58 [LP epoch 03] train_loss=1.0416 train_acc=0.5099 | val_loss=4884.4620 val_acc=0.5350 | 
02-19 14:57:07 [LP epoch 04] train_loss=1.0283 train_acc=0.5175 | val_loss=4827.2452 val_acc=0.5355 | 
02-19 14:57:15 [LP epoch 05] train_loss=1.0170 train_acc=0.5251 | val_loss=4790.3981 val_acc=0.5487 | 
02-19 14:57:23 [LP epoch 06] train_loss=1.0070 train_acc=0.5327 | val_loss=4737.9373 val_acc=0.5494 | 
02-19 14:57:33 [LP epoch 07] train_loss=0.9985 train_acc=0.5408 | val_loss=4698.7569 val_acc=0.5626 | 
02-19 14:57:42 [LP epoch 08] train_loss=0.9914 train_acc=0.5487 | val_loss=4662.9556 val_acc=0.5682 | 
02-19 14:57:51 [LP epoch 09] train_loss=0.9857 train_acc=0.5541 | val_loss=4629.4015 val_acc=0.5738 | 
02-19 14:58:00 [LP epoch 10] train_loss=0.9806 train_acc=0.5575 | val_loss=4613.5282 val_acc=0.5765 | 
02-19 14:58:08 [LP epoch 11] train_loss=0.9761 train_acc=0.5612 | val_loss=4593.3503 val_acc=0.5753 | 
02-19 14:58:18 [LP epoch 12] train_loss=0.9724 train_acc=0.5646 | val_loss=4574.8260 val_acc=0.5790 | 
02-19 14:58:26 [LP epoch 13] train_loss=0.9690 train_acc=0.5662 | val_loss=4551.0319 val_acc=0.5846 | 
02-19 14:58:35 [LP epoch 14] train_loss=0.9657 train_acc=0.5696 | val_loss=4548.8999 val_acc=0.5888 | 
02-19 14:58:43 [LP epoch 15] train_loss=0.9628 train_acc=0.5726 | val_loss=4539.9059 val_acc=0.5898 | 
02-19 14:58:51 [LP epoch 16] train_loss=0.9602 train_acc=0.5730 | val_loss=4503.9037 val_acc=0.5962 | 
02-19 14:59:00 [LP epoch 17] train_loss=0.9575 train_acc=0.5752 | val_loss=4516.0612 val_acc=0.5921 | 
02-19 14:59:10 [LP epoch 18] train_loss=0.9549 train_acc=0.5779 | val_loss=4483.3684 val_acc=0.5958 | 
02-19 14:59:19 [LP epoch 19] train_loss=0.9524 train_acc=0.5798 | val_loss=4468.6373 val_acc=0.6008 | 
02-19 14:59:28 [LP epoch 20] train_loss=0.9504 train_acc=0.5809 | val_loss=4469.9530 val_acc=0.5983 | 
02-19 14:59:37 [LP epoch 21] train_loss=0.9484 train_acc=0.5818 | val_loss=4458.7741 val_acc=0.6024 | 
02-19 14:59:46 [LP epoch 22] train_loss=0.9462 train_acc=0.5842 | val_loss=4437.7852 val_acc=0.6024 | 
02-19 14:59:55 [LP epoch 23] train_loss=0.9441 train_acc=0.5867 | val_loss=4422.4076 val_acc=0.6043 | 
02-19 15:00:03 [LP epoch 24] train_loss=0.9424 train_acc=0.5883 | val_loss=4412.6320 val_acc=0.6072 | 
02-19 15:00:11 [LP epoch 25] train_loss=0.9405 train_acc=0.5879 | val_loss=4399.5651 val_acc=0.6087 | 
02-19 15:00:21 [LP epoch 26] train_loss=0.9384 train_acc=0.5904 | val_loss=4383.7157 val_acc=0.6072 | 
02-19 15:00:29 [LP epoch 27] train_loss=0.9364 train_acc=0.5895 | val_loss=4372.8848 val_acc=0.6064 | 
02-19 15:00:38 [LP epoch 28] train_loss=0.9346 train_acc=0.5913 | val_loss=4352.6483 val_acc=0.6109 | 
02-19 15:00:47 [LP epoch 29] train_loss=0.9323 train_acc=0.5940 | val_loss=4322.4528 val_acc=0.6126 | 
02-19 15:00:55 [LP epoch 30] train_loss=0.9299 train_acc=0.5955 | val_loss=4310.9733 val_acc=0.6139 | 
02-19 15:01:04 [LP epoch 31] train_loss=0.9278 train_acc=0.5967 | val_loss=4300.2025 val_acc=0.6147 | 
02-19 15:01:13 [LP epoch 32] train_loss=0.9252 train_acc=0.5980 | val_loss=4273.5283 val_acc=0.6165 | 
02-19 15:01:22 [LP epoch 33] train_loss=0.9223 train_acc=0.5994 | val_loss=4251.5554 val_acc=0.6197 | 
02-19 15:01:30 [LP epoch 34] train_loss=0.9197 train_acc=0.6021 | val_loss=4230.8697 val_acc=0.6207 | 
02-19 15:01:38 [LP epoch 35] train_loss=0.9173 train_acc=0.6041 | val_loss=4197.0192 val_acc=0.6304 | 
02-19 15:01:46 [LP epoch 36] train_loss=0.9152 train_acc=0.6054 | val_loss=4190.8743 val_acc=0.6321 | 
02-19 15:01:54 [LP epoch 37] train_loss=0.9131 train_acc=0.6055 | val_loss=4176.9578 val_acc=0.6317 | 
02-19 15:02:03 [LP epoch 38] train_loss=0.9112 train_acc=0.6063 | val_loss=4177.9645 val_acc=0.6315 | 
02-19 15:02:12 [LP epoch 39] train_loss=0.9096 train_acc=0.6067 | val_loss=4155.0725 val_acc=0.6352 | 
02-19 15:02:21 [LP epoch 40] train_loss=0.9083 train_acc=0.6074 | val_loss=4150.4951 val_acc=0.6367 | 
02-19 15:02:31 [LP epoch 41] train_loss=0.9071 train_acc=0.6063 | val_loss=4148.8091 val_acc=0.6360 | 
02-19 15:02:39 [LP epoch 42] train_loss=0.9059 train_acc=0.6083 | val_loss=4132.0769 val_acc=0.6377 | 
02-19 15:02:47 [LP epoch 43] train_loss=0.9047 train_acc=0.6087 | val_loss=4129.1135 val_acc=0.6344 | 
02-19 15:02:56 [LP epoch 44] train_loss=0.9041 train_acc=0.6097 | val_loss=4112.1510 val_acc=0.6363 | 
02-19 15:03:04 [LP epoch 45] train_loss=0.9031 train_acc=0.6080 | val_loss=4116.4347 val_acc=0.6414 | 
02-19 15:03:13 [LP epoch 46] train_loss=0.9020 train_acc=0.6097 | val_loss=4118.1507 val_acc=0.6377 | 
02-19 15:03:22 [LP epoch 47] train_loss=0.9014 train_acc=0.6107 | val_loss=4108.9819 val_acc=0.6369 | 
02-19 15:03:31 [LP epoch 48] train_loss=0.9010 train_acc=0.6099 | val_loss=4094.8819 val_acc=0.6402 | 
02-19 15:03:40 [LP epoch 49] train_loss=0.9000 train_acc=0.6114 | val_loss=4104.4946 val_acc=0.6379 | 
02-19 15:03:40 Saved linear-probe checkpoint: ./checkpoint\SSF_PU_0219-145022\linear_probe_best.pt (best_val_acc=0.6414)
02-19 15:03:41 TEST linear-probe: loss=0.8424 acc=0.6328
02-19 15:03:41 using 1 gpus
02-19 15:03:41 Dataset class: <class 'data_utils.datasets.PU.PU'>
02-19 15:03:42 Split sizes: train=22502 val=4822 test=4823
02-19 15:03:43 Label counts train: Counter({1: 8481, 3: 7718, 0: 4201, 2: 2102})
02-19 15:03:43 Label counts val:   Counter({1: 1817, 3: 1654, 0: 900, 2: 451})
02-19 15:03:44 Label counts test:  Counter({1: 1818, 3: 1654, 0: 901, 2: 450})
02-19 15:03:46 -----Epoch 0/19-----
